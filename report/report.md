# Background

Benchmark test code that are generated by LLM
select a case/feature, generate multiple test code by multiple LLM

Create a leader board

## Activities
### First priority: reate a script/pipeline that
Before executing larger experimentation, I might be useful to have a pipeline and toolchains ready
Start with small dataset but end-to-end ready
Build a pipeline that can
1. Read a codebase
2. Automatically generate unit test
   - for LLM-based generator: have a prompting template for each LLM and run the prompt (see [2]). Possible tools
     - Bare minimum 1: Github's copilot
     - Bare minimum 2: Codex (will it be the same to copilot?)
     - Bare minimum 3: StarCoder (Big Code Community's, open source LLM trained on github's permissive projects)
     - Code Llama (Meta's)
     - Gemini (Google's)
   - for non-LLM-based generator: run script to trigger the generation. Possible tools 
     - Evosuite (https://www.evosuite.org/ based on search based software testing?)
     - Randoop (https://randoop.github.io/randoop/ based on feedback directed random testing?)
     - others nice to have, see [3]
     
   note that we use non-LLM based generator simply as baseline benchmark/comparison. Similar to what [1] did
3. Run benchmarking test on the generated test code
   - Bare minimum: compile, test success, and coverage: by Jacoco, trigger by gradle
   - other metrics? (i.e. test mutation, test minimisation, etc.)
   - nice to have: code smell by linter (gradle's checkstyle). Nice to have because different codebase have different rules
4. Sort the result, create a leaderboard

### Develop initial fake codebase
just to test the pipeline. With small and controllable dataset. 
Later, we can run the pipeline on real codebase or use existing dataset such as HumanEval
with initial 3 types of code
1. simple branching: fizzbuzz
2. recursion flow: fibonacci
3. has mocking components: simple CRUD operation on user registrar, with features
   - check user, return boolean, true if exists
   - user login, supply username and password; return user id if success; throw exception if not exists; throw exception if wrong password
   - create new user; if exists throw exception, else return the user object
   - delete user; return void if success; throw exception if user is not found


### References
#### [1] M. L. Siddiq, J. C. S. Santos, R. H. Tanvir, N. Ulfat, F. A. Rifat, and V. C. Lopes, “Exploring the Effectiveness of Large Language Models in Generating Unit Tests,” Apr. 2023, arXiv:2305.00418.
- Generate unit test, classification code type and apply heuristic to make the generation works
- follow how they benchmark the generated code
- but focus on building the pipeline
- initially this project tries to reproduce the activity in this paper
- but might exclude the heuristic process (or safe for later when we use real codebase/dataset)

#### [2] Z. Yuan, Y. Lou, M. Liu, S. Ding, K. Wang, Y. Chen, and X. Peng, “No More Manual Tests? Evaluating and Improving ChatGPT for Unit Test Generation,” May 2023, arXiv:2305.04207.
- ChatTester, developed prompting for unit test
- follow some prompting guide presented

#### [3]  W. Ma, S. Liu, W. Wang, Q. Hu, Y. Liu, C. Zhang, L. Nie, and Y. Liu, “The scope of ChatGPT in software engineering: A thorough investigation,” 2023, arXiv:2305.12138.
list of existing methodology and tools for test generation (that is not LLM-based)


## questions
- how to access copilot programmatically? best if possible, if not we need to manually collect the generated test case
- access to other code generator?
- Java or javascript? (if javascript, gradle will be replaced by node)